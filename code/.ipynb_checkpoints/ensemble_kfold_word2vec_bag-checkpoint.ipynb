{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teruto\\Desktop\\sourcecode\\livedoor-news-clustering\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teruto\\Desktop\\sourcecode\\livedoor-news-clustering\\data\n"
     ]
    }
   ],
   "source": [
    "cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ファイル読み込み\n",
    "train_df = pd.read_csv('./preprosessing_train.csv')\n",
    "X_df_train = train_df[['text_length', 'is_year', 'is_month', 'is_date',\n",
    "       'is_time', 'num_hatena', 'num_bikkuri', 'is_eiga', 'is_henkakko',\n",
    "       'is_normalkakko', 'is_whitekakko', 'is_vol', 'is_sports',\n",
    "       'text_del_one']]\n",
    "Y_df_train = train_df[\"flg\"].astype(np.int64)\n",
    "X_train =  X_df_train.values\n",
    "t_train = Y_df_train.values\n",
    "\n",
    "dev_df = pd.read_csv('./preprosessing_dev.csv')\n",
    "X_df_dev = dev_df[['text_length', 'is_year', 'is_month', 'is_date',\n",
    "       'is_time', 'num_hatena', 'num_bikkuri', 'is_eiga', 'is_henkakko',\n",
    "       'is_normalkakko', 'is_whitekakko', 'is_vol', 'is_sports',\n",
    "       'text_del_one']]\n",
    "Y_df_dev = dev_df[\"flg\"].astype(np.int64)\n",
    "X_valid =  X_df_dev.values\n",
    "t_valid = Y_df_dev.values\n",
    "\n",
    "test_df = pd.read_csv('./preprosessing_test.csv')\n",
    "X_df_test = test_df[['text_length', 'is_year', 'is_month', 'is_date',\n",
    "       'is_time', 'num_hatena', 'num_bikkuri', 'is_eiga', 'is_henkakko',\n",
    "       'is_normalkakko', 'is_whitekakko', 'is_vol', 'is_sports',\n",
    "       'text_del_one']]\n",
    "Y_df_test = test_df[\"flg\"].astype(np.int64)\n",
    "X_test =  X_df_test.values\n",
    "t_test = Y_df_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徴量配列とテキスト配列に分ける\n",
    "X_train_feat = X_train[:,:-1]\n",
    "X_valid_feat = X_valid[:,:-1]\n",
    "X_test_feat = X_test[:,:-1]\n",
    "X_train_feat.shape\n",
    "X_train_text = X_train[:,-1]\n",
    "X_valid_text = X_valid[:,-1]\n",
    "X_test_text = X_test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徴量標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "ss = MinMaxScaler()\n",
    "X_train_feat_ss = ss.fit_transform(X_train_feat)\n",
    "X_valid_feat_ss = ss.transform(X_valid_feat)\n",
    "X_test_feat_ss = ss.transform(X_test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文字カラムに対する欠損値削除\n",
    "X_train_del_one = X_train_text\n",
    "for idx in range(len(X_train_del_one)):\n",
    "    if type(X_train_del_one[idx]) is float:\n",
    "        X_train_del_one[idx] = \"Null\" \n",
    "\n",
    "X_valid_del_one = X_valid_text\n",
    "for idx in range(len(X_valid_del_one)):\n",
    "    if type(X_valid_del_one[idx]) is float:\n",
    "        X_valid_del_one[idx] = \"Null\"\n",
    "        \n",
    "X_test_del_one = X_test_text\n",
    "for idx in range(len(X_test_del_one)):\n",
    "    if type(X_test_del_one[idx]) is float:\n",
    "        X_test_del_one[idx] = \"Null\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of words による単語ベクトル化(最大値、最小値を設定した)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "cv = TfidfVectorizer(min_df=10/5000, max_df=2000/5000)\n",
    "#cv = TfidfVectorizer()\n",
    "X_train_cv = cv.fit_transform(X_train_del_one).toarray().astype(np.float32)\n",
    "X_valid_cv = cv.transform(X_valid_del_one).toarray().astype(np.float32)\n",
    "X_test_cv = cv.transform(X_test_del_one).toarray().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vecによる単語ベクトル化(モデルのロード)\n",
    "import gensim\n",
    "model= gensim.models.KeyedVectors.load_word2vec_format('jawiki.word_vectors.300d.abtt.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vocab = set(model.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "c:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "#word2vecによる単語ベクトル化(モデルのロード)\n",
    "def swem_avg(word_list,model, model_vocab):\n",
    "    ave_arr = np.mean(np.array([model[word] for word in word_list.split(\" \") if word in model_vocab]), axis=0)\n",
    "    if np.shape(ave_arr) != (300,):\n",
    "        return np.zeros((1,300))\n",
    "    return ave_arr.reshape([1,300])\n",
    "\n",
    "X_train_w2v = np.zeros((1,300))\n",
    "for text in X_train_del_one:\n",
    "    X_train_w2v = np.concatenate([X_train_w2v,swem_avg(text, model, model_vocab)])\n",
    "X_train_w2v = X_train_w2v[1:len(X_train_w2v)].astype(np.float32)\n",
    "X_valid_w2v = np.zeros((1,300))\n",
    "for text in X_valid_del_one:\n",
    "    X_valid_w2v = np.concatenate([X_valid_w2v,swem_avg(text, model, model_vocab)])\n",
    "X_valid_w2v = X_valid_w2v[1:len(X_valid_w2v)].astype(np.float32)\n",
    "X_test_w2v = np.zeros((1,300))\n",
    "for text in X_test_del_one:\n",
    "    X_test_w2v = np.concatenate([X_test_w2v,swem_avg(text, model, model_vocab)])\n",
    "X_test_w2v = X_test_w2v[1:len(X_test_w2v)].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 300)\n",
      "(500, 300)\n",
      "(500, 300)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train_w2v))\n",
    "print(np.shape(X_valid_w2v))\n",
    "print(np.shape(X_test_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_w2v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_train = np.concatenate([X_train_feat_ss,X_train_w2v],1)\\nX_valid = np.concatenate([X_valid_feat_ss,X_valid_w2v],1)\\nX_test = np.concatenate([X_test_feat_ss,X_test_w2v],1)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#文字配列と特徴量の配列を結合する\n",
    "\n",
    "X_train = np.concatenate([X_train_cv,X_train_feat_ss,X_train_w2v],1)\n",
    "X_valid = np.concatenate([X_valid_cv,X_valid_feat_ss,X_valid_w2v],1)\n",
    "X_test = np.concatenate([X_test_cv,X_test_feat_ss,X_test_w2v],1)\n",
    "\"\"\"\n",
    "X_train = np.concatenate([X_train_feat_ss,X_train_w2v],1)\n",
    "X_valid = np.concatenate([X_valid_feat_ss,X_valid_w2v],1)\n",
    "X_test = np.concatenate([X_test_feat_ss,X_test_w2v],1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#型変換\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_valid = X_valid.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "t_train = t_train.astype(np.int32)\n",
    "t_valid = t_valid.astype(np.int32)\n",
    "t_test = t_test.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#スパース配列に変換\n",
    "from scipy.sparse import csr_matrix\n",
    "X_train_csr = csr_matrix(X_train)\n",
    "X_valid_csr = csr_matrix(X_valid)\n",
    "X_test_csr = csr_matrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#混合行列\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score ,precision_score, recall_score,f1_score\n",
    "import seaborn as sns\n",
    "def result_heatmap(Y_test,Y_pred):\n",
    "    print(\"正解率:\"+str(accuracy_score(Y_test, Y_pred)))\n",
    "    cm = confusion_matrix(Y_test, Y_pred)\n",
    "    #print(cm)\n",
    "    sns.heatmap(cm,annot=True, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_train_pca = X_train_csr.toarray()\\nX_valid_pca = X_valid_csr.toarray()\\nX_test_pca = X_test_csr.toarray()\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA\n",
    "from sklearn.decomposition import  TruncatedSVD \n",
    "pca =  TruncatedSVD(500)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train_csr) \n",
    "X_valid_pca = pca.transform(X_valid_csr)\n",
    "X_test_pca = pca.transform(X_test_csr)\n",
    "\"\"\"\n",
    "X_train_pca = X_train_csr.toarray()\n",
    "X_valid_pca = X_valid_csr.toarray()\n",
    "X_test_pca = X_test_csr.toarray()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class S1Model():\n",
    "    def __init__(self,name):\n",
    "        self.models = []\n",
    "        self.name = name\n",
    "    \n",
    "\n",
    "        \n",
    "    def fit_proba(self,X_train,y_train,X_valid):\n",
    "        model = self.get_classifier(self.name)\n",
    "        model.fit(X_train,y_train)\n",
    "        self.models.append(model)\n",
    "        return model.predict_proba(X_valid)\n",
    "    \n",
    "    def predict(self,X_test):  \n",
    "        ave_arr = np.zeros((len(X_test),9))\n",
    "        for model in self.models:\n",
    "            proba = model.predict_proba(X_test)\n",
    "            np.add(ave_arr,proba)\n",
    "            proba = model.predict_proba(X_test_pca)\n",
    "            ave_arr = np.add(ave_arr,proba)\n",
    "        return ave_arr/len(self.models)\n",
    "    \n",
    "    def get_classifier(self,name):\n",
    "        if name == \"svm1\":\n",
    "            return svm.SVC(kernel=\"rbf\",gamma = 0.01,C=50,probability=True)\n",
    "        elif name == \"svm2\":\n",
    "            return svm.SVC(kernel=\"linear\",gamma = 10,C=10,probability=True)\n",
    "        elif name == \"rfc1\":\n",
    "            return RandomForestClassifier(n_estimators=100,max_depth = 3,criterion=\"entropy\")\n",
    "        elif name == \"rfc2\":\n",
    "            return RandomForestClassifier(n_estimators=500,max_depth = 5,criterion= \"gini\")\n",
    "        elif name == \"mlp\":\n",
    "            return MLPClassifier(hidden_layer_sizes=(256,128,64,32),activation=\"relu\",solver = \"adam\",max_iter=10000,early_stopping=True)\n",
    "        elif name == \"lr\":\n",
    "            return LogisticRegression(C=300,random_state=1)\n",
    "        elif name == \"gbc\":\n",
    "            return GradientBoostingClassifier(random_state=0)\n",
    "        elif name == \"abc\":\n",
    "            return AdaBoostClassifier(base_estimator=DecisionTreeClassifier(random_state=0), n_estimators=100, learning_rate=1,  random_state=None)\n",
    "        elif name == \"k-nei1\":\n",
    "            return  KNeighborsClassifier(n_neighbors=5)\n",
    "        elif name == \"k-nei2\":\n",
    "            return  KNeighborsClassifier(n_neighbors=20)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_epoc\n",
      "svm1done\n",
      "svm2done\n",
      "rfc1done\n",
      "rfc2done\n",
      "mlpdone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrdone\n",
      "k-nei1done\n",
      "k-nei2done\n",
      "gbcdone\n",
      "abcdone\n",
      "new_epoc\n",
      "svm1done\n",
      "svm2done\n",
      "rfc1done\n",
      "rfc2done\n",
      "mlpdone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrdone\n",
      "k-nei1done\n",
      "k-nei2done\n",
      "gbcdone\n",
      "abcdone\n",
      "new_epoc\n",
      "svm1done\n",
      "svm2done\n",
      "rfc1done\n",
      "rfc2done\n",
      "mlpdone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrdone\n",
      "k-nei1done\n",
      "k-nei2done\n",
      "gbcdone\n",
      "abcdone\n",
      "new_epoc\n",
      "svm1done\n",
      "svm2done\n",
      "rfc1done\n",
      "rfc2done\n",
      "mlpdone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrdone\n",
      "k-nei1done\n",
      "k-nei2done\n",
      "gbcdone\n",
      "abcdone\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from copy import deepcopy\n",
    "\n",
    "S1_models = [S1Model(\"svm1\"),S1Model(\"svm2\"),S1Model(\"rfc1\"),S1Model(\"rfc2\"),S1Model(\"mlp\"),S1Model(\"lr\"),S1Model(\"k-nei1\"),S1Model(\"k-nei2\"),S1Model(\"gbc\"),S1Model(\"abc\")]\n",
    "\n",
    "X_valids = np.zeros((1,9*len(S1_models)))#9はラベルの種類\n",
    "y_valids = np.zeros(1)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4)#ここのランダム大事よ\n",
    "for train_idx, valid_idx in skf.split(X_train_pca,t_train):\n",
    "    print(\"new_epoc\")\n",
    "    X_train = X_train_pca[train_idx]\n",
    "    y_train = t_train[train_idx]\n",
    "    X_valid = X_train_pca[valid_idx]\n",
    "    y_valid = t_train[valid_idx]\n",
    "    temp_X_valids = np.zeros((np.shape(X_valid)[0],1))\n",
    "    for model in S1_models:\n",
    "        proba = model.fit_proba(X_train,y_train,X_valid)\n",
    "        temp_X_valids = np.concatenate([temp_X_valids,proba],1)\n",
    "        print(model.name+\"done\")\n",
    "    #結合\n",
    "    X_valids = np.concatenate([X_valids,temp_X_valids[:,1:len(temp_X_valids)]],0)\n",
    "    y_valids = np.append(y_valids, y_valid)\n",
    "X_valids = X_valids[1:np.shape(X_valids)[0],:]\n",
    "y_valids = y_valids[1:len(y_valids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('ensemble_kfold_X.txt', X_valids)\n",
    "np.savetxt('ensemble_kfold_y.txt', y_valids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_valids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=2, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=10, kernel='linear',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S2_model =svm.SVC(kernel=\"linear\",gamma = 10,C=2,probability=True)\n",
    "S2_model.fit(X_valids,y_valids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm1\n",
      "svm2\n",
      "rfc1\n",
      "rfc2\n",
      "mlp\n",
      "lr\n",
      "k-nei1\n",
      "k-nei2\n",
      "gbc\n",
      "abc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_S2_test = np.zeros((np.shape(X_test_pca )[0],1)) \n",
    "for S1_model in S1_models:\n",
    "    proba = S1_model.predict(X_test_pca)\n",
    "    print(S1_model.name)\n",
    "    X_S2_test = np.concatenate([X_S2_test,proba],1)\n",
    "X_S2_test= X_S2_test[:,1:len(X_S2_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score ,precision_score, recall_score,f1_score\n",
    "import seaborn as sns\n",
    "def result_heatmap(Y_test,Y_pred):\n",
    "    print(\"正解率:\"+str(accuracy_score(Y_test, Y_pred)))\n",
    "    cm = confusion_matrix(Y_test, Y_pred)\n",
    "    #print(cm)\n",
    "    sns.heatmap(cm,annot=True, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解率:0.798\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-98321489c38b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mt_S2_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mS2_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_S2_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresult_heatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt_S2_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-b3cb98d53bef>\u001b[0m in \u001b[0;36mresult_heatmap\u001b[1;34m(Y_test, Y_pred)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"正解率:\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#cm = confusion_matrix(Y_test, Y_pred)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mannot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Blues'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cm' is not defined"
     ]
    }
   ],
   "source": [
    "t_S2_pred = S2_model.predict(X_S2_test)\n",
    "result_heatmap(t_test,t_S2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
