{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teruto\\Desktop\\livedoor-news-clustering\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ファイル読み込み\n",
    "train_df = pd.read_csv('./data/preprosessing_train.csv')\n",
    "X_df_train = train_df[['text_length', 'is_year', 'is_month', 'is_date',\n",
    "       'is_time', 'num_hatena', 'num_bikkuri', 'is_eiga', 'is_henkakko',\n",
    "       'is_normalkakko', 'is_whitekakko', 'is_vol', 'is_sports',\n",
    "       'text_del_one']]\n",
    "Y_df_train = train_df[\"flg\"].astype(np.int64)\n",
    "X_train =  X_df_train.values\n",
    "t_train = Y_df_train.values\n",
    "\n",
    "dev_df = pd.read_csv('./data/preprosessing_dev.csv')\n",
    "X_df_dev = dev_df[['text_length', 'is_year', 'is_month', 'is_date',\n",
    "       'is_time', 'num_hatena', 'num_bikkuri', 'is_eiga', 'is_henkakko',\n",
    "       'is_normalkakko', 'is_whitekakko', 'is_vol', 'is_sports',\n",
    "       'text_del_one']]\n",
    "Y_df_dev = dev_df[\"flg\"].astype(np.int64)\n",
    "X_valid =  X_df_dev.values\n",
    "t_valid = Y_df_dev.values\n",
    "\n",
    "test_df = pd.read_csv('./data/preprosessing_test.csv')\n",
    "X_df_test = test_df[['text_length', 'is_year', 'is_month', 'is_date',\n",
    "       'is_time', 'num_hatena', 'num_bikkuri', 'is_eiga', 'is_henkakko',\n",
    "       'is_normalkakko', 'is_whitekakko', 'is_vol', 'is_sports',\n",
    "       'text_del_one']]\n",
    "Y_df_test = test_df[\"flg\"].astype(np.int64)\n",
    "X_test =  X_df_test.values\n",
    "t_test = Y_df_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徴量配列とテキスト配列に分ける\n",
    "X_train_feat = X_train[:,:-1]\n",
    "X_valid_feat = X_valid[:,:-1]\n",
    "X_test_feat = X_test[:,:-1]\n",
    "X_train_feat.shape\n",
    "X_train_text = X_train[:,-1]\n",
    "X_valid_text = X_valid[:,-1]\n",
    "X_test_text = X_test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徴量標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "ss = MinMaxScaler()\n",
    "X_train_feat_ss = ss.fit_transform(X_train_feat)\n",
    "X_valid_feat_ss = ss.transform(X_valid_feat)\n",
    "X_test_feat_ss = ss.transform(X_test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文字カラムに対する欠損値削除\n",
    "X_train_del_one = X_train_text\n",
    "for idx in range(len(X_train_del_one)):\n",
    "    if type(X_train_del_one[idx]) is float:\n",
    "        X_train_del_one[idx] = \"Null\" \n",
    "\n",
    "X_valid_del_one = X_valid_text\n",
    "for idx in range(len(X_valid_del_one)):\n",
    "    if type(X_valid_del_one[idx]) is float:\n",
    "        X_valid_del_one[idx] = \"Null\"\n",
    "        \n",
    "X_test_del_one = X_test_text\n",
    "for idx in range(len(X_test_del_one)):\n",
    "    if type(X_test_del_one[idx]) is float:\n",
    "        X_test_del_one[idx] = \"Null\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of words による単語ベクトル化(最大値、最小値を設定した)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "cv = TfidfVectorizer(min_df=3/5000, max_df=3000/5000)\n",
    "#cv = TfidfVectorizer()\n",
    "X_train_cv = cv.fit_transform(X_train_del_one).toarray().astype(np.float32)\n",
    "X_valid_cv = cv.transform(X_valid_del_one).toarray().astype(np.float32)\n",
    "X_test_cv = cv.transform(X_test_del_one).toarray().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vecによる単語ベクトル化(モデルのロード)\n",
    "import gensim\n",
    "model= gensim.models.KeyedVectors.load_word2vec_format('./w2v/jawiki.word_vectors.300d.abtt.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vocab = set(model.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "c:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "#word2vecによる単語ベクトル化(モデルのロード)\n",
    "def swem_avg(word_list,model, model_vocab):\n",
    "    ave_arr = np.mean(np.array([model[word] for word in word_list.split(\" \") if word in model_vocab]), axis=0)\n",
    "    if np.shape(ave_arr) != (300,):\n",
    "        return np.zeros((1,300))\n",
    "    return ave_arr.reshape([1,300])\n",
    "\n",
    "X_train_w2v = np.zeros((1,300))\n",
    "for text in X_train_del_one:\n",
    "    X_train_w2v = np.concatenate([X_train_w2v,swem_avg(text, model, model_vocab)])\n",
    "X_train_w2v = X_train_w2v[1:len(X_train_w2v)].astype(np.float32)\n",
    "X_valid_w2v = np.zeros((1,300))\n",
    "for text in X_valid_del_one:\n",
    "    X_valid_w2v = np.concatenate([X_valid_w2v,swem_avg(text, model, model_vocab)])\n",
    "X_valid_w2v = X_valid_w2v[1:len(X_valid_w2v)].astype(np.float32)\n",
    "X_test_w2v = np.zeros((1,300))\n",
    "for text in X_test_del_one:\n",
    "    X_test_w2v = np.concatenate([X_test_w2v,swem_avg(text, model, model_vocab)])\n",
    "X_test_w2v = X_test_w2v[1:len(X_test_w2v)].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 300)\n",
      "(500, 300)\n",
      "(500, 300)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train_w2v))\n",
    "print(np.shape(X_valid_w2v))\n",
    "print(np.shape(X_test_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_w2v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_train = np.concatenate([X_train_feat_ss,X_train_w2v],1)\\nX_valid = np.concatenate([X_valid_feat_ss,X_valid_w2v],1)\\nX_test = np.concatenate([X_test_feat_ss,X_test_w2v],1)\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#文字配列と特徴量の配列を結合する\n",
    "\n",
    "X_train = np.concatenate([X_train_cv,X_train_feat_ss,X_train_w2v],1)\n",
    "X_valid = np.concatenate([X_valid_cv,X_valid_feat_ss,X_valid_w2v],1)\n",
    "X_test = np.concatenate([X_test_cv,X_test_feat_ss,X_test_w2v],1)\n",
    "\"\"\"\n",
    "X_train = np.concatenate([X_train_feat_ss,X_train_w2v],1)\n",
    "X_valid = np.concatenate([X_valid_feat_ss,X_valid_w2v],1)\n",
    "X_test = np.concatenate([X_test_feat_ss,X_test_w2v],1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 63.9 MiB for an array with shape (5000, 3350) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-242053aeb269>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#型変換\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mt_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 63.9 MiB for an array with shape (5000, 3350) and data type float32"
     ]
    }
   ],
   "source": [
    "#型変換\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_valid = X_valid.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "t_train = t_train.astype(np.int32)\n",
    "t_valid = t_valid.astype(np.int32)\n",
    "t_test = t_test.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#スパース配列に変換\n",
    "from scipy.sparse import csr_matrix\n",
    "X_train_csr = csr_matrix(X_train)\n",
    "X_valid_csr = csr_matrix(X_valid)\n",
    "X_test_csr = csr_matrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#混合行列\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score ,precision_score, recall_score,f1_score\n",
    "import seaborn as sns\n",
    "def result_heatmap(Y_test,Y_pred):\n",
    "    print(\"正解率:\"+str(accuracy_score(Y_test, Y_pred)))\n",
    "    cm = confusion_matrix(Y_test, Y_pred)\n",
    "    #print(cm)\n",
    "    sns.heatmap(cm,annot=True, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_train_pca = X_train_csr.toarray()\\nX_valid_pca = X_valid_csr.toarray()\\nX_test_pca = X_test_csr.toarray()\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA\n",
    "from sklearn.decomposition import  TruncatedSVD \n",
    "pca =  TruncatedSVD(500)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train_csr) \n",
    "X_valid_pca = pca.transform(X_valid_csr)\n",
    "X_test_pca = pca.transform(X_test_csr)\n",
    "\"\"\"\n",
    "X_train_pca = X_train_csr.toarray()\n",
    "X_valid_pca = X_valid_csr.toarray()\n",
    "X_test_pca = X_test_csr.toarray()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class S1Model():\n",
    "    def __init__(self,name):\n",
    "        self.models = []\n",
    "        self.name = name\n",
    "    \n",
    "\n",
    "        \n",
    "    def fit_proba(self,X_train,y_train,X_valid):\n",
    "        model = self.get_classifier(self.name)\n",
    "        model.fit(X_train,y_train)\n",
    "        self.models.append(model)\n",
    "        return model.predict_proba(X_valid)\n",
    "    \n",
    "    def predict(self,X_test):  \n",
    "        ave_arr = np.zeros((len(X_test),9))\n",
    "        for model in self.models:\n",
    "            proba = model.predict_proba(X_test)\n",
    "            np.add(ave_arr,proba)\n",
    "            proba = model.predict_proba(X_test_pca)\n",
    "            ave_arr = np.add(ave_arr,proba)\n",
    "        return ave_arr/len(self.models)\n",
    "    \n",
    "    def get_classifier(self,name):\n",
    "        if name == \"svm1\":\n",
    "            return svm.SVC(kernel=\"rbf\",gamma = 0.01,C=50,probability=True)\n",
    "        elif name == \"svm2\":\n",
    "            return svm.SVC(kernel=\"linear\",gamma = 10,C=10,probability=True)\n",
    "        elif name == \"rfc1\":\n",
    "            return RandomForestClassifier(n_estimators=100,max_depth = 3,criterion=\"entropy\")\n",
    "        elif name == \"rfc2\":\n",
    "            return RandomForestClassifier(n_estimators=500,max_depth = 5,criterion= \"gini\")\n",
    "        elif name == \"mlp\":\n",
    "            return MLPClassifier(hidden_layer_sizes=(256,128,64,32),activation=\"relu\",solver = \"adam\",max_iter=10000,early_stopping=True)\n",
    "        elif name == \"lr\":\n",
    "            return LogisticRegression(C=300,random_state=1)\n",
    "        elif name == \"gbc\":\n",
    "            return GradientBoostingClassifier(random_state=0)\n",
    "        elif name == \"abc\":\n",
    "            return AdaBoostClassifier(base_estimator=DecisionTreeClassifier(random_state=0), n_estimators=100, learning_rate=1,  random_state=None)\n",
    "        elif name == \"k-nei1\":\n",
    "            return  KNeighborsClassifier(n_neighbors=5)\n",
    "        elif name == \"k-nei2\":\n",
    "            return  KNeighborsClassifier(n_neighbors=20)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_epoc\n",
      "svm1done\n",
      "svm2done\n",
      "rfc1done\n",
      "rfc2done\n",
      "mlpdone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrdone\n",
      "k-nei1done\n",
      "k-nei2done\n",
      "gbcdone\n",
      "abcdone\n",
      "new_epoc\n",
      "svm1done\n",
      "svm2done\n",
      "rfc1done\n",
      "rfc2done\n",
      "mlpdone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrdone\n",
      "k-nei1done\n",
      "k-nei2done\n",
      "gbcdone\n",
      "abcdone\n",
      "new_epoc\n",
      "svm1done\n",
      "svm2done\n",
      "rfc1done\n",
      "rfc2done\n",
      "mlpdone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrdone\n",
      "k-nei1done\n",
      "k-nei2done\n",
      "gbcdone\n",
      "abcdone\n",
      "new_epoc\n",
      "svm1done\n",
      "svm2done\n",
      "rfc1done\n",
      "rfc2done\n",
      "mlpdone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrdone\n",
      "k-nei1done\n",
      "k-nei2done\n",
      "gbcdone\n",
      "abcdone\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from copy import deepcopy\n",
    "\n",
    "S1_models = [S1Model(\"svm1\"),S1Model(\"svm2\"),S1Model(\"rfc1\"),S1Model(\"rfc2\"),S1Model(\"mlp\"),S1Model(\"lr\"),S1Model(\"k-nei1\"),S1Model(\"k-nei2\"),S1Model(\"gbc\"),S1Model(\"abc\")]\n",
    "\n",
    "X_blends = np.zeros((1,9*len(S1_models)))#9はラベルの種類\n",
    "y_blends = np.zeros(1)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4)#ここのランダム大事よ\n",
    "for train_idx, valid_idx in skf.split(X_train_pca,t_train):\n",
    "    print(\"new_epoc\")\n",
    "    X_train = X_train_pca[train_idx]\n",
    "    y_train = t_train[train_idx]\n",
    "    X_valid = X_train_pca[blend_idx]\n",
    "    y_valid = t_train[blend_idx]\n",
    "    temp_X_blends = np.zeros((np.shape(X_valid)[0],1))\n",
    "    for model in S1_models:\n",
    "        proba = model.fit_proba(X_train,y_train,X_valid)\n",
    "        temp_X_blends = np.concatenate([temp_X_blends,proba],1)\n",
    "        print(model.name+\"done\")\n",
    "    #結合\n",
    "    X_blends = np.concatenate([X_blends,temp_X_blends[:,1:len(temp_X_blends)]],0)\n",
    "    y_blends = np.append(y_blends, y_valid)\n",
    "X_blends = X_blends[1:np.shape(X_blends)[0],:]\n",
    "y_blends = y_blends[1:len(y_blends)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_valids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1090)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=300, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2層目\n",
    "#best: S2_model =svm.SVC(kernel=\"rbf\",degree = 10,gamma = 0.01,C=10,probability=True)\n",
    "S2_model =LogisticRegression(C=300,random_state=1)\n",
    "X_S1_train = np.concatenate([X_blends,X_train_pca],1)\n",
    "print(np.shape(X_valids))\n",
    "S2_model.fit(X_S1_train,y_blends)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm1\n",
      "svm2\n",
      "rfc1\n",
      "rfc2\n",
      "mlp\n",
      "lr\n",
      "k-nei1\n",
      "k-nei2\n",
      "gbc\n",
      "abc\n"
     ]
    }
   ],
   "source": [
    "#Stage1の推論\n",
    "X_S2_test = np.zeros((np.shape(X_test_pca )[0],1)) \n",
    "for S1_model in S1_models:\n",
    "    proba = S1_model.predict(X_test_pca)\n",
    "    print(S1_model.name)\n",
    "    X_S2_test = np.concatenate([X_S2_test,proba],1)\n",
    "X_S2_test= X_S2_test[:,1:len(X_S2_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score ,precision_score, recall_score,f1_score\n",
    "import seaborn as sns\n",
    "def result_heatmap(Y_test,Y_pred):\n",
    "    print(\"正解率:\"+str(accuracy_score(Y_test, Y_pred)))\n",
    "    cm = confusion_matrix(Y_test, Y_pred)\n",
    "    #print(cm)\n",
    "    sns.heatmap(cm,annot=True, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 590 features per sample; expecting 1090",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-daeaf1c5b288>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX_S2_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_S2_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test_pca\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mt_S2_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mS2_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_S2_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mresult_heatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt_S2_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \"\"\"\n\u001b[1;32m--> 293\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[1;32m--> 273\u001b[1;33m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[1;31mValueError\u001b[0m: X has 590 features per sample; expecting 1090"
     ]
    }
   ],
   "source": [
    "\n",
    "X_S2_test = np.concatenate([X_S2_test,X_test_pca],1)\n",
    "t_S2_pred = S2_model.predict(X_S2_test)\n",
    "result_heatmap(t_test,t_S2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
