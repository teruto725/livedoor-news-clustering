{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomforest svm_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teruto\\Desktop\\sourcecode\\livedoor-news-clustering\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teruto\\Desktop\\sourcecode\\livedoor-news-clustering\\data\n"
     ]
    }
   ],
   "source": [
    "cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ファイル読み込み\n",
    "train_df = pd.read_csv('./preprosessing_train.csv')\n",
    "X_df_train = train_df[['text_length', 'is_year', 'is_month', 'is_date',\n",
    "       'is_time', 'num_hatena', 'num_bikkuri', 'is_eiga', 'is_henkakko',\n",
    "       'is_normalkakko', 'is_whitekakko', 'is_vol', 'is_sports',\n",
    "       'text_del_one']]\n",
    "Y_df_train = train_df[\"flg\"].astype(np.int64)\n",
    "X_train =  X_df_train.values\n",
    "t_train = Y_df_train.values\n",
    "\n",
    "dev_df = pd.read_csv('./preprosessing_dev.csv')\n",
    "X_df_dev = dev_df[['text_length', 'is_year', 'is_month', 'is_date',\n",
    "       'is_time', 'num_hatena', 'num_bikkuri', 'is_eiga', 'is_henkakko',\n",
    "       'is_normalkakko', 'is_whitekakko', 'is_vol', 'is_sports',\n",
    "       'text_del_one']]\n",
    "Y_df_dev = dev_df[\"flg\"].astype(np.int64)\n",
    "X_valid =  X_df_dev.values\n",
    "t_valid = Y_df_dev.values\n",
    "\n",
    "test_df = pd.read_csv('./preprosessing_test.csv')\n",
    "X_df_test = test_df[['text_length', 'is_year', 'is_month', 'is_date',\n",
    "       'is_time', 'num_hatena', 'num_bikkuri', 'is_eiga', 'is_henkakko',\n",
    "       'is_normalkakko', 'is_whitekakko', 'is_vol', 'is_sports',\n",
    "       'text_del_one']]\n",
    "Y_df_test = test_df[\"flg\"].astype(np.int64)\n",
    "X_test =  X_df_test.values\n",
    "t_test = Y_df_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#特徴量配列\n",
    "X_train_feat = X_train[:,:-1]\n",
    "X_valid_feat = X_valid[:,:-1]\n",
    "X_test_feat = X_test[:,:-1]\n",
    "X_train_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徴量標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "ss = MinMaxScaler()\n",
    "X_train_feat_ss = ss.fit_transform(X_train_feat)\n",
    "X_valid_feat_ss = ss.transform(X_valid_feat)\n",
    "X_test_feat_ss = ss.transform(X_test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文字カラムに対する欠損値削除\n",
    "X_train_del_one = X_train[:,-1]\n",
    "for idx in range(len(X_train_del_one)):\n",
    "    if type(X_train_del_one[idx]) is float:\n",
    "        X_train_del_one[idx] = \" \" \n",
    "\n",
    "X_valid_del_one = X_valid[:,-1]\n",
    "for idx in range(len(X_valid_del_one)):\n",
    "    if type(X_valid_del_one[idx]) is float:\n",
    "        X_valid_del_one[idx] = \" \"\n",
    "        \n",
    "X_test_del_one = X_test[:,-1]\n",
    "for idx in range(len(X_test_del_one)):\n",
    "    if type(X_test_del_one[idx]) is float:\n",
    "        X_test_del_one[idx] = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#単語ベクトル化(最大値、最小値を設定した)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "cv = TfidfVectorizer(min_df=3/5000, max_df=3800/5000)\n",
    "#cv = TfidfVectorizer()\n",
    "X_train_cv = cv.fit_transform(X_train_del_one)\n",
    "X_valid_cv = cv.transform(X_valid_del_one)\n",
    "X_test_cv = cv.transform(X_test_del_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文字配列と特徴量の配列を結合する\n",
    "X_train = np.concatenate([X_train_cv.toarray(),X_train_feat_ss],1)\n",
    "X_valid = np.concatenate([X_valid_cv.toarray(),X_valid_feat_ss],1)\n",
    "X_test = np.concatenate([X_test_cv.toarray(),X_test_feat_ss],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#型変換\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_valid = X_valid.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "t_train = t_train.astype(np.int32)\n",
    "t_valid = t_valid.astype(np.int32)\n",
    "t_test = t_test.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#スパース配列に変換\n",
    "from scipy.sparse import csr_matrix\n",
    "X_train_csr = csr_matrix(X_train)\n",
    "X_valid_csr = csr_matrix(X_valid)\n",
    "X_test_csr = csr_matrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#混合行列\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score ,precision_score, recall_score,f1_score\n",
    "import seaborn as sns\n",
    "def result_heatmap(Y_test,Y_pred):\n",
    "    print(\"正解率:\"+str(accuracy_score(Y_test, Y_pred)))\n",
    "    cm = confusion_matrix(Y_test, Y_pred)\n",
    "    #print(cm)\n",
    "    sns.heatmap(cm,annot=True, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "from sklearn.decomposition import  TruncatedSVD \n",
    "pca =  TruncatedSVD(500)\n",
    "X_train_pca = pca.fit_transform(X_train_csr) \n",
    "X_valid_pca = pca.transform(X_valid_csr)\n",
    "X_test_pca = pca.transform(X_test_csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#randomforest\n",
    "from sklearn import svm\n",
    "model = SVC(kernel='linear', random_state=None)\n",
    "model.fit(X_train_pca, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pred = model.predict(X_test_pca)\n",
    "result_heatmap(t_test,t_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
