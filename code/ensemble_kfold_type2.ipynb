{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teruto\\Desktop\\livedoor-news-clustering\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ファイル読み込み\n",
    "train_df = pd.read_csv('./data/preprosessing_train.csv')\n",
    "X_df_train = train_df[['text_length', 'is_year', 'is_month', 'is_date',\n",
    "       'is_time', 'num_hatena', 'num_bikkuri', 'is_eiga', 'is_henkakko',\n",
    "       'is_normalkakko', 'is_whitekakko', 'is_vol', 'is_sports',\n",
    "       'text_del_one']]\n",
    "Y_df_train = train_df[\"flg\"].astype(np.int64)\n",
    "X_train =  X_df_train.values\n",
    "t_train = Y_df_train.values\n",
    "\n",
    "dev_df = pd.read_csv('./data/preprosessing_dev.csv')\n",
    "X_df_dev = dev_df[['text_length', 'is_year', 'is_month', 'is_date',\n",
    "       'is_time', 'num_hatena', 'num_bikkuri', 'is_eiga', 'is_henkakko',\n",
    "       'is_normalkakko', 'is_whitekakko', 'is_vol', 'is_sports',\n",
    "       'text_del_one']]\n",
    "Y_df_dev = dev_df[\"flg\"].astype(np.int64)\n",
    "X_valid =  X_df_dev.values\n",
    "t_valid = Y_df_dev.values\n",
    "\n",
    "test_df = pd.read_csv('./data/preprosessing_test.csv')\n",
    "X_df_test = test_df[['text_length', 'is_year', 'is_month', 'is_date',\n",
    "       'is_time', 'num_hatena', 'num_bikkuri', 'is_eiga', 'is_henkakko',\n",
    "       'is_normalkakko', 'is_whitekakko', 'is_vol', 'is_sports',\n",
    "       'text_del_one']]\n",
    "Y_df_test = test_df[\"flg\"].astype(np.int64)\n",
    "X_test =  X_df_test.values\n",
    "t_test = Y_df_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徴量配列とテキスト配列に分ける\n",
    "X_train_feat = X_train[:,:-1]\n",
    "X_valid_feat = X_valid[:,:-1]\n",
    "X_test_feat = X_test[:,:-1]\n",
    "X_train_feat.shape\n",
    "X_train_text = X_train[:,-1]\n",
    "X_valid_text = X_valid[:,-1]\n",
    "X_test_text = X_test[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徴量標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "ss = MinMaxScaler()\n",
    "X_train_feat_ss = ss.fit_transform(X_train_feat)\n",
    "X_valid_feat_ss = ss.transform(X_valid_feat)\n",
    "X_test_feat_ss = ss.transform(X_test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文字カラムに対する欠損値削除\n",
    "X_train_del_one = X_train_text\n",
    "for idx in range(len(X_train_del_one)):\n",
    "    if type(X_train_del_one[idx]) is float:\n",
    "        X_train_del_one[idx] = \"Null\" \n",
    "\n",
    "X_valid_del_one = X_valid_text\n",
    "for idx in range(len(X_valid_del_one)):\n",
    "    if type(X_valid_del_one[idx]) is float:\n",
    "        X_valid_del_one[idx] = \"Null\"\n",
    "        \n",
    "X_test_del_one = X_test_text\n",
    "for idx in range(len(X_test_del_one)):\n",
    "    if type(X_test_del_one[idx]) is float:\n",
    "        X_test_del_one[idx] = \"Null\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of words による単語ベクトル化(最大値、最小値を設定した)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "cv = TfidfVectorizer(min_df=3/5000, max_df=3000/5000)\n",
    "#cv = TfidfVectorizer()\n",
    "X_train_cv = cv.fit_transform(X_train_del_one).toarray().astype(np.float32)\n",
    "X_valid_cv = cv.transform(X_valid_del_one).toarray().astype(np.float32)\n",
    "X_test_cv = cv.transform(X_test_del_one).toarray().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vecによる単語ベクトル化(モデルのロード)\n",
    "import gensim\n",
    "model= gensim.models.KeyedVectors.load_word2vec_format('./w2v/jawiki.word_vectors.300d.abtt.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vocab = set(model.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "c:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "#word2vecによる単語ベクトル化(モデルのロード)\n",
    "def swem_avg(word_list,model, model_vocab):\n",
    "    ave_arr = np.mean(np.array([model[word] for word in word_list.split(\" \") if word in model_vocab]), axis=0)\n",
    "    if np.shape(ave_arr) != (300,):\n",
    "        return np.zeros((1,300))\n",
    "    return ave_arr.reshape([1,300])\n",
    "\n",
    "X_train_w2v = np.zeros((1,300))\n",
    "for text in X_train_del_one:\n",
    "    X_train_w2v = np.concatenate([X_train_w2v,swem_avg(text, model, model_vocab)])\n",
    "X_train_w2v = X_train_w2v[1:len(X_train_w2v)].astype(np.float32)\n",
    "X_valid_w2v = np.zeros((1,300))\n",
    "for text in X_valid_del_one:\n",
    "    X_valid_w2v = np.concatenate([X_valid_w2v,swem_avg(text, model, model_vocab)])\n",
    "X_valid_w2v = X_valid_w2v[1:len(X_valid_w2v)].astype(np.float32)\n",
    "X_test_w2v = np.zeros((1,300))\n",
    "for text in X_test_del_one:\n",
    "    X_test_w2v = np.concatenate([X_test_w2v,swem_avg(text, model, model_vocab)])\n",
    "X_test_w2v = X_test_w2v[1:len(X_test_w2v)].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 300)\n",
      "(500, 300)\n",
      "(500, 300)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train_w2v))\n",
    "print(np.shape(X_valid_w2v))\n",
    "print(np.shape(X_test_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_w2v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 128. MiB for an array with shape (5000, 3350) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-d161f10dda87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#文字配列と特徴量の配列を結合する\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_train_cv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train_feat_ss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train_w2v\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mX_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_valid_cv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_valid_feat_ss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_valid_w2v\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_test_cv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test_feat_ss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test_w2v\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 128. MiB for an array with shape (5000, 3350) and data type float64"
     ]
    }
   ],
   "source": [
    "#文字配列と特徴量の配列を結合する\n",
    "\n",
    "X_train = np.concatenate([X_train_cv,X_train_feat_ss,X_train_w2v],1)\n",
    "X_valid = np.concatenate([X_valid_cv,X_valid_feat_ss,X_valid_w2v],1)\n",
    "X_test = np.concatenate([X_test_cv,X_test_feat_ss,X_test_w2v],1)\n",
    "\"\"\"\n",
    "X_train = np.concatenate([X_train_feat_ss,X_train_w2v],1)\n",
    "X_valid = np.concatenate([X_valid_feat_ss,X_valid_w2v],1)\n",
    "X_test = np.concatenate([X_test_feat_ss,X_test_w2v],1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '本田 ｲﾁﾛｰ 似る 作文 話題 必ず 世界一 なる '",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-242053aeb269>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#型変換\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mt_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '本田 ｲﾁﾛｰ 似る 作文 話題 必ず 世界一 なる '"
     ]
    }
   ],
   "source": [
    "#型変換\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_valid = X_valid.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "t_train = t_train.astype(np.int32)\n",
    "t_valid = t_valid.astype(np.int32)\n",
    "t_test = t_test.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "no supported conversion for types: (dtype('O'),)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36masformat\u001b[1;34m(self, format, copy)\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\scipy\\sparse\\coo.py\u001b[0m in \u001b[0;36mtocsr\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\scipy\\sparse\\sputils.py\u001b[0m in \u001b[0;36mupcast\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'no supported conversion for types: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: no supported conversion for types: (dtype('O'),)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-ef48da915f0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#スパース配列に変換\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_train_csr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mX_valid_csr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_test_csr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     86\u001b[0m                                  \"\".format(self.format))\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcoo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;31m# Read matrix dimensions given, if any\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[0marg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                 \u001b[0marg1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36masformat\u001b[1;34m(self, format, copy)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[1;31m###################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\scipy\\sparse\\coo.py\u001b[0m in \u001b[0;36mtocsr\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    404\u001b[0m             \u001b[0mindptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m             coo_tocsr(M, N, self.nnz, row, col, self.data,\n",
      "\u001b[1;32mc:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\scipy\\sparse\\sputils.py\u001b[0m in \u001b[0;36mupcast\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'no supported conversion for types: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: no supported conversion for types: (dtype('O'),)"
     ]
    }
   ],
   "source": [
    "\n",
    "#スパース配列に変換\n",
    "from scipy.sparse import csr_matrix\n",
    "X_train_csr = csr_matrix(X_train)\n",
    "X_valid_csr = csr_matrix(X_valid)\n",
    "X_test_csr = csr_matrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#混合行列\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score ,precision_score, recall_score,f1_score\n",
    "import seaborn as sns\n",
    "def result_heatmap(Y_test,Y_pred):\n",
    "    print(\"正解率:\"+str(accuracy_score(Y_test, Y_pred)))\n",
    "    cm = confusion_matrix(Y_test, Y_pred)\n",
    "    #print(cm)\n",
    "    sns.heatmap(cm,annot=True, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_train_pca = X_train_csr.toarray()\\nX_valid_pca = X_valid_csr.toarray()\\nX_test_pca = X_test_csr.toarray()\\n'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA\n",
    "from sklearn.decomposition import  TruncatedSVD \n",
    "pca =  TruncatedSVD(500)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train_csr) \n",
    "X_valid_pca = pca.transform(X_valid_csr)\n",
    "X_test_pca = pca.transform(X_test_csr)\n",
    "\"\"\"\n",
    "X_train_pca = X_train_csr.toarray()\n",
    "X_valid_pca = X_valid_csr.toarray()\n",
    "X_test_pca = X_test_csr.toarray()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class S1Model():\n",
    "    def __init__(self,name):\n",
    "        self.models = []\n",
    "        self.name = name\n",
    "    \n",
    "\n",
    "        \n",
    "    def fit_proba(self,X_train,y_train,X_valid):\n",
    "        model = self.get_classifier(self.name)\n",
    "        model.fit(X_train,y_train)\n",
    "        self.models.append(model)\n",
    "        return model.predict_proba(X_valid)\n",
    "    \n",
    "    def predict(self,X_test):  \n",
    "        ave_arr = np.zeros((len(X_test),9))\n",
    "        for model in self.models:\n",
    "            proba = model.predict_proba(X_test)\n",
    "            np.add(ave_arr,proba)\n",
    "            proba = model.predict_proba(X_test_pca)\n",
    "            ave_arr = np.add(ave_arr,proba)\n",
    "        return ave_arr/len(self.models)\n",
    "    \n",
    "    def get_classifier(self,name):\n",
    "        if name == \"svm1\":\n",
    "            return svm.SVC(kernel=\"rbf\",gamma = 0.01,C=50,probability=True)\n",
    "        elif name == \"svm2\":\n",
    "            return svm.SVC(kernel=\"linear\",gamma = 10,C=10,probability=True)\n",
    "        elif name == \"rfc1\":\n",
    "            return RandomForestClassifier(n_estimators=100,max_depth = 3,criterion=\"entropy\")\n",
    "        elif name == \"rfc2\":\n",
    "            return RandomForestClassifier(n_estimators=500,max_depth = 5,criterion= \"gini\")\n",
    "        elif name == \"mlp\":\n",
    "            return MLPClassifier(hidden_layer_sizes=(256,128,64,32),activation=\"relu\",solver = \"adam\",max_iter=10000,early_stopping=True)\n",
    "        elif name == \"lr\":\n",
    "            return LogisticRegression(C=300,random_state=1)\n",
    "        elif name == \"gbc\":\n",
    "            return GradientBoostingClassifier(random_state=0)\n",
    "        elif name == \"abc\":\n",
    "            return AdaBoostClassifier(base_estimator=DecisionTreeClassifier(random_state=0), n_estimators=100, learning_rate=1,  random_state=None)\n",
    "        elif name == \"k-nei1\":\n",
    "            return  KNeighborsClassifier(n_neighbors=5)\n",
    "        elif name == \"k-nei2\":\n",
    "            return  KNeighborsClassifier(n_neighbors=20)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "svm1done\n",
      "svm2done\n",
      "rfc1done\n",
      "rfc2done\n",
      "mlpdone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrdone\n",
      "k-nei1done\n",
      "k-nei2done\n",
      "gbcdone\n",
      "abcdone\n",
      "-------------------\n",
      "svm1done\n",
      "svm2done\n",
      "rfc1done\n",
      "rfc2done\n",
      "mlpdone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrdone\n",
      "k-nei1done\n",
      "k-nei2done\n",
      "gbcdone\n",
      "abcdone\n",
      "-------------------\n",
      "svm1done\n",
      "svm2done\n",
      "rfc1done\n",
      "rfc2done\n",
      "mlpdone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrdone\n",
      "k-nei1done\n",
      "k-nei2done\n",
      "gbcdone\n",
      "abcdone\n",
      "-------------------\n",
      "svm1done\n",
      "svm2done\n",
      "rfc1done\n",
      "rfc2done\n",
      "mlpdone\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\teruto\\desktop\\sourcecode\\livedoor-news-clustering\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrdone\n",
      "k-nei1done\n",
      "k-nei2done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from copy import deepcopy\n",
    "\n",
    "S1_models = [S1Model(\"svm1\"),S1Model(\"svm2\"),S1Model(\"rfc1\"),S1Model(\"rfc2\"),S1Model(\"mlp\"),S1Model(\"lr\"),S1Model(\"k-nei1\"),S1Model(\"k-nei2\"),S1Model(\"gbc\"),S1Model(\"abc\")]\n",
    "\n",
    "X_blends = np.zeros((1,9*len(S1_models)))#9はラベルの種類\n",
    "y_blends = np.zeros(1)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4)#ここのランダム大事よ\n",
    "for train_idx, valid_idx in skf.split(X_train_pca,t_train):\n",
    "    print(\"-------------------\")\n",
    "    X_train = X_train_pca[train_idx]\n",
    "    y_train = t_train[train_idx]\n",
    "    X_valid = X_train_pca[valid_idx]\n",
    "    y_valid = t_train[valid_idx]\n",
    "    temp_X_blends = np.zeros((np.shape(X_valid)[0],1))\n",
    "    for model in S1_models:\n",
    "        proba = model.fit_proba(X_train,y_train,X_valid)\n",
    "        temp_X_blends = np.concatenate([temp_X_blends,proba],1)\n",
    "        print(model.name+\"done\")\n",
    "    #結合\n",
    "    X_blends = np.concatenate([X_blends,temp_X_blends[:,1:len(temp_X_blends)]],0)\n",
    "    y_blends = np.append(y_blends, y_valid)\n",
    "X_blends = X_blends[1:np.shape(X_blends)[0],:]\n",
    "y_blends = y_blends[1:len(y_blends)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(y_valids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2層目\n",
    "#best: S2_model =svm.SVC(kernel=\"rbf\",degree = 10,gamma = 0.01,C=10,probability=True)\n",
    "S2_model =LogisticRegression(C=300,random_state=1)\n",
    "X_S1_train = np.concatenate([X_blends,X_train_pca],1)\n",
    "print(np.shape(X_valids))\n",
    "S2_model.fit(X_S1_train,y_blends)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage1の推論\n",
    "X_S2_test = np.zeros((np.shape(X_test_pca )[0],1)) \n",
    "for S1_model in S1_models:\n",
    "    proba = S1_model.predict(X_test_pca)\n",
    "    print(S1_model.name)\n",
    "    X_S2_test = np.concatenate([X_S2_test,proba],1)\n",
    "X_S2_test= X_S2_test[:,1:len(X_S2_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score ,precision_score, recall_score,f1_score\n",
    "import seaborn as sns\n",
    "def result_heatmap(Y_test,Y_pred):\n",
    "    print(\"正解率:\"+str(accuracy_score(Y_test, Y_pred)))\n",
    "    cm = confusion_matrix(Y_test, Y_pred)\n",
    "    #print(cm)\n",
    "    sns.heatmap(cm,annot=True, cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_S2_test = np.concatenate([X_S2_test,X_test_pca],1)\n",
    "t_S2_pred = S2_model.predict(X_S2_test)\n",
    "result_heatmap(t_test,t_S2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
